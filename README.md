# 혼공머신 
- 용어설명
  - 특성(feature): 생선 데이터는 [길이, 무게]의 2개의 특성으로 나타내었다.
  - 모델: 머신러닝 알고리즘이 구현된 객체를 뜻한다.모델 클래스를 추정기(estimator) 라고도함
  - 정확도: 맞힌 갯수/전체 데이터 수
  - train_set, test_set: 모델 훈련용 데이터와 이를 검증하는 테스트 데이터로 나눈다
  - data_set: input + target이 결합된 학습 데이터 셋이다.
  - 지도학습(supervised_learning): 입력과 타깃을 모델에 학습시켜 새로운 데이터를 예측하는것에 활용
  - 비지도학습(un-): 입력 데이터만 학습하기 때문에 예측하지 않음, 데이터 특징 파악에 활용
  - 강화학습(reinforcement -): 타깃없이 알고리즘의 결과로 얻는 보상을 학습에 사용
  - sampling bias: 예측할 데이터의 샘플이 골고루 섞여있지 않고 특정 타깃의 샘플을 과도하게 학습 시킬 경우 생기는 현상
  - data_preprocessing: 특성간의 기준이 달라 생기는 오차를 없애기위해 특성값의 스케일을 일정한 기준으로 맞추어주는 작업 
  - Standard_score: z-score라고도한다.특성값($x$)에서 데이터들의 평균($\mu$)을 빼고 표준편차($\sigma$)를 나눈 표준점수($z$)를 사용해서 스케일할수있다. $$z = {x-\mu\over\sigma}$$
  - Broad_casting: 넘파이 배열에서 연산을 자동으로 모든 행과 열로 확장해주는 기능
  - 회귀: 두 변수 사이의 상관관계를 분석하여 한 변수가 변할때 다른 변수는 어떻게 변하는가 파악하는 방법
  - 평균절대오차 (mean_absolute_error:MAE)
  실제값 $y$,예측값 $\hat{y}$의 오차들의 합을 평균한것
  $$MAE(y,\hat{y}) = {1\over n } \sum^n_{i=1}\vert y_i - \hat{y_i} \vert$$
  - 과대적합(overfitting): 훈련한 데이터의 $R^2$점수가 테스트용 데이터 $R^2$보다 높다면 과대적합되었다 라고한다.
  - 과소적합(underfitting): 두점수가 모두 낮거나, 테스트 점수가 너무 높다면 모델이 훈련셋에 과소적합되었다고 한다.
  - 선형회귀(linear regression): 최적의 직선,1차방정식, 선형적으로 데이터를 예측함
  - 다항회귀(polynomial-): 2차방정식, 다항식을 사용한 선형회귀, 최적의 곡선
  - 다중회귀(multiple-): 여러개의 특성을 사용하는 선형회귀이다
  => 특성이 두개가 되면 선형회귀모델은 평면을 학습한다.
  - 기울기 = 계수(coefficient) = 가중치(weight)
  - 사례기반학습: 모델 파라미터가 없음. 훈련세트를 그대로 저장하는 모델
  - 모델기반학습: 머신러닝알고리즘이 찾은 기울기계수나, 절편값을 모델 파라미터라고 부르며 최적을 모델 파라미터를 찾는것이 목표이다.
  - 특성공학(feature engineering): 기존의 특성을 사용해 새로운 특성을 뽑아내는 작업
  - 사이킷런의 변환기(transfomer): 특성을 만들거나. 전처리하는 클래스이다.
  - 규제(regularization): 특성공학으로 특성을 마구 늘리면 완벽히 학습하여 과대적합된다. 이를 방지하는 방법이 규제이다. 선형회귀에서 계수의 크기를 줄이는 작업, 
  - 릿지(ridge): 계수를 제곱한 값을 기준으로 규제를 적용
  - 라쏘(lasso): 계수의 절댓값을 기준으로 규제를 적용 => 계수값을 0으로 만들어서 유용한 특성을 골라내는데 사용할 수 있다.
  - Hyperparameter: 수동으로 지정해줘야하는 모델클래스의 매개변수









- K-Nearest Neighbors Classifier(분류)
  - 예측할 데이터의 특성과 가까운 데이터의 타겟값을 예측으로 출력한다.(클래스 분류)
  - 학습이라기보단 단순히 데이터간의 거리를 비교해서 구하는 것이라 비효율적이다.
- K-Nearest Neighbors Regression(회귀)
  - 주변 샘플의 수치를 평균하여 타깃값을 예측
  - 회귀는 정답을 맞추는게 불가능하다. 따라서 정확도(score)를 결정계수 $R^2$ 로 평가한다 $$R^2 = 1-{(타깃-예측)^2의 합\over (타깃-평균)^2의 합}$$
  => 예측이 타깃에 가까워질수록 $R^2$는 1에 가까워진다.
